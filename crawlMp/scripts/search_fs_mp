#!/usr/bin/env python3
import argparse
import multiprocessing
import os
import signal
import sys

from crawlMp import __version__
from crawlMp.constants import *
from crawlMp.crawlMp import CrawlMp
from crawlMp.crawlers.fileCrawler import FileSearchCrawler
from crawlMp.results import Results
from crawlMp.snippets.output import print_summary, print_list

description = [
    f"crawlMp v{__version__}",
    "MultiProcess recursive file search.",
    "Default starting location is Current Working Directory.",
    "",
    "Usage examples:",
    "  Search for all .zip files:",
    "  search_fs_mp \\\\.zip$",
    "",
    "  Get all .zip files in different directories:",
    "  search_fs_mp \\\\.zip$ -l /home /usr/share",
    "",
    "  Show search summary:",
    "  search_fs_mp \\\\.zip$ -l /home /usr/share -os",
    "",
]

arguments = sys.argv
parser = argparse.ArgumentParser(
    formatter_class=argparse.RawTextHelpFormatter,
    description="\r\n".join(description)
)
parser.add_argument("pattern", default=".", type=str, help="RegExp filename pattern to search for")
parser.add_argument("-l", "--links", type=str, nargs="+",
                    help="Entry point(s) to start search from.", default=[os.getcwd()])
parser.add_argument("-o", "--output", default=[OUTPUT_LIST], type=str, nargs="+", choices=[OUTPUT_SUMMARY, OUTPUT_LIST],
                    help=f"Print search result:\r\n  l: list of hits (default)\r\n  s: short summary")
parser.add_argument("-np", "--processes", default=multiprocessing.cpu_count(), type=int,
                    help="Number of processes used, minimum is 1")
parser.add_argument("-v", "--version", help="Show crawlMp version", action='store_true')
parser.add_argument("-bs", "--buffer_size", default=96, type=int, help="Buffer links size, only used if processes > 1")
args = parser.parse_args()

if args.version:
    print(f"crawlMp v{__version__}")


def on_done(crawler_results: Results) -> None:
    for output_mode in args.output:
        if output_mode == OUTPUT_SUMMARY:
            print_summary(crawler_results)
        elif output_mode == OUTPUT_LIST:
            print_list(crawler_results)


manager = CrawlMp(FileSearchCrawler, links=args.links, num_proc=args.processes, buffer_size=args.buffer_size,
                  pattern=args.pattern, mode=MODE_SIMPLE)
signal.signal(signal.SIGINT, lambda sig, frame: manager.stop())
manager.start(on_done)
